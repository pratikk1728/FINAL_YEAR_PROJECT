# -*- coding: utf-8 -*-
"""DETECT_main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bglTpIcdX8ePInC8Ge2t9tEOhbpBlFNw
"""

pip install ultralytics opencv-python

from ultralytics import YOLO
import os
import yaml

# Load the pretrained YOLOv8 model
model = YOLO('yolov8n.pt')  # or 'yolov8s.pt', 'yolov8m.pt', 'yolov8l.pt', 'yolov8x.pt' based on your needs

!pip install roboflow

from roboflow import Roboflow
rf = Roboflow(api_key="4afexldZYJEBSduTP9Yz")
project = rf.workspace("guna-agkd3").project("lab1-jbyhl")
version = project.version(1)
dataset = version.download("yolov8")

# Train the model
model.train(data='/content/Lab1-1/data.yaml', epochs=10, batch=16, imgsz=640, project='weapon_detection', name='yolov8_weapon', exist_ok=True)

# Evaluate the trained model
metrics = model.val()
print(metrics)

# Run inference
results = model('/content/Gun_2.jpg')
results[0].show()  # Display the results
results[0].save()  # Save the result image

# Run inference
results = model('/content/GUN.webp')
results[0].show()  # Display the results
results[0].save()  # Save the result image

import cv2
import numpy as np
from ultralytics import YOLO
from collections import deque

# Load the fine-tuned model
model = YOLO('/content/weapon_detection/yolov8_weapon/weights/best.pt')  # Replace with your model path

# Video setup
video_path = '/content/G.I. Joe Retaliation (2013) - Weapons Time Scene (1080p) FULL HD (online-video-cutter.com).mp4'  # Input video path
output_path = '/content/fucku.mp4'  # Output video path

# Open video capture and get properties
cap = cv2.VideoCapture(video_path)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))

# Define codec and create VideoWriter object for output
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

# Initialize a queue to store results across frames for smoothing
detections_queue = deque(maxlen=5)  # Holds detections from last 5 frames

# Process each frame in the video
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Run YOLO inference with custom thresholds
    results = model(frame, conf=0.5, iou=0.4)  # Increase confidence threshold, lower IoU threshold if needed

    # Extract detections and filter them
    detections = []
    for result in results[0].boxes:
        if result.conf.item() > 0.5:  # Check confidence
            # Append bounding box, class, and confidence to detections
            x1, y1, x2, y2 = map(int, result.xyxy[0])  # Get box coordinates
            class_id = int(result.cls.item())  # Get class ID
            conf = result.conf.item()  # Get confidence score
            detections.append((x1, y1, x2, y2, class_id, conf))

    # Add detections for this frame to the queue for smoothing
    detections_queue.append(detections)

    # Smooth detections across frames
    smoothed_detections = []
    for i in range(len(detections)):
        # Calculate average box coordinates and confidence over stored frames
        x1_avg = np.mean([det[i][0] for det in detections_queue if len(det) > i])
        y1_avg = np.mean([det[i][1] for det in detections_queue if len(det) > i])
        x2_avg = np.mean([det[i][2] for det in detections_queue if len(det) > i])
        y2_avg = np.mean([det[i][3] for det in detections_queue if len(det) > i])
        conf_avg = np.mean([det[i][5] for det in detections_queue if len(det) > i])

        # Only keep the detection if the averaged confidence is above threshold
        if conf_avg > 0.5:
            smoothed_detections.append((int(x1_avg), int(y1_avg), int(x2_avg), int(y2_avg), conf_avg))

    # Draw smoothed detections on the frame
    for (x1, y1, x2, y2, conf) in smoothed_detections:
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(frame, f'Weapon: {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)


    out.write(frame)  # Write frame to output video

    # Press 'q' to exit
    # if cv2.waitKey(1) & 0xFF == ord('q'):
    #     break

# Release resources
cap.release()
out.release()
# cv2.destroyAllWindows()

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

!pip install Pillow # install Pillow, if not already installed
from IPython.display import display, Javascript
from google.colab.output import eval_js
from PIL import Image # Import the Image class from Pillow
import cv2
import numpy as np
import base64 # Import base64
from ultralytics import YOLO
from IPython.display import clear_output
# Load the YOLO model (Replace with your model path if fine-tuned)
model = YOLO('/content/weapon_detection/yolov8_weapon/weights/best.pt')  # Or use your fine-tuned model path like 'runs/detect/yolov8_weapon/weights/best.pt'

# JavaScript code to access webcam
def get_webcam_frame():
    js = Javascript('''
    async function captureWebcam() {
        const video = document.createElement('video');
        video.style.display = 'none';
        document.body.appendChild(video);

        // Request webcam permission explicitly
        const stream = await navigator.mediaDevices.getUserMedia({video: true})
            .catch(err => {
                console.error("Error accessing webcam:", err);
                return null; // Return null if permission is denied
            });

        if (!stream) {
            return null; // Return null if stream is not available
        }

        video.srcObject = stream;
        await video.play();

        // Resize video to 640x480 for faster processing
        const canvas = document.createElement('canvas');
        canvas.width = 640;
        canvas.height = 480;
        const context = canvas.getContext('2d');

        // Capture frame to canvas
        context.drawImage(video, 0, 0, canvas.width, canvas.height);
        stream.getTracks().forEach(track => track.stop());
        video.remove();

        return canvas.toDataURL('image/jpeg', 0.8); // Compress image
    }
    captureWebcam();
    ''')
    display(js)

    # Decode the JavaScript base64 image and convert to numpy array
    data = eval_js('captureWebcam()')

    # Handle case where webcam access is denied (data is null)
    if data is None:
        print("Webcam access denied. Please grant permission.")
        return None

    img_bytes = base64.b64decode(data.split(',')[1])
    np_arr = np.frombuffer(img_bytes, np.uint8)
    img = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)
    return img

# Code to process and display webcam feed with YOLOv8
try:
    while True:
        # Capture frame from webcam
        frame = get_webcam_frame()

        # Check if frame was captured successfully
        if frame is None:
            break  # Exit the loop if webcam access is denied

        # Run YOLOv8 detection on the frame
        results = model(frame, conf=0.5, iou=0.4)  # Customize conf and iou as needed

        # Draw boxes and labels on the frame
        annotated_frame = results[0].plot()

        # Display the frame in real-time
        _, buffer = cv2.imencode('.jpg', annotated_frame)
        clear_output(wait=True)  # Clear previous output before displaying new frame
        # Use Image.fromarray to create a PIL Image object from the NumPy array
        display(Image.fromarray(annotated_frame)) # Use display directly on Image


except KeyboardInterrupt:
    print("Detection stopped.")

